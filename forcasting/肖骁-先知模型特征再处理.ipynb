{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time \n",
    "from pyspark.sql.types import StructType \n",
    "from pyspark.sql.types import StructField\n",
    "import pyspark.sql.types as sql_type\n",
    "name = locals()\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/bin/python3\"\n",
    "\n",
    "spark = (SparkSession\\\n",
    "         .builder\\\n",
    "         .appName(\"test\")\\\n",
    "         .enableHiveSupport()\\\n",
    "         .config(\"spark.executor.instances\", \"200\")\\\n",
    "         .config(\"spark.executor.memory\",\"16g\")\\\n",
    "         .config(\"spark.executor.cores\",\"4\")\\\n",
    "         .config(\"spark.driver.memory\",\"40g\")\\\n",
    "         .config(\"spark.sql.shuffle.partitions\",\"800\")\\\n",
    "         .config(\"spark.default.parallelism\",\"800\")\\\n",
    "         .config(\"spark.driver.maxResultSize\", \"8g\")\\\n",
    "         .config(\"spark.pyspark.python\", \"/usr/bin/python3\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "import spa_utils\n",
    "\n",
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions=200551''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions.pernode=200551''')\n",
    "params = {'author':'xiaoxiao10'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error,f1_score,precision_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.spatial.distance import cosine\n",
    "name = locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql('''select * from dev.xianzhi_v2_basedata_1''').filter(F.col('item_sku_id')=='100000022911')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 该版本目标：仅通过price这唯一features来进行预测，将全部流程跑通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.select('item_sku_id','dt','netprice','qtty')\n",
    "df_1_pandas = df_1.toPandas()\n",
    "df_1_pandas['netprice'] = df_1_pandas['netprice'].map(lambda x: 60 if x <0.1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_pandas_1 = df_1_pandas.sort_values('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_pandas_2  = df_1_pandas_1.rename(columns={'netprice':'target_price','qtty':'target_qtty'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile('SPA_simulation_functions.py')\n",
    "\n",
    "selected_columns = ['item_sku_id', 'dt','target_price','target_qtty','test_flag','valid_flag']\n",
    "X_SCHEMA_SKU = ['target_price']\n",
    "rolling_columns = []\n",
    "SCHEMA_OUTPUT_SKU = StructType([\n",
    "    StructField(\"dt\", sql_type.StringType()),\n",
    "    StructField(\"item_sku_id\", sql_type.StringType()),\n",
    "    StructField(\"target_price\", sql_type.FloatType()),\n",
    "    StructField(\"prediction\", sql_type.FloatType()),\n",
    "    StructField(\"r2_predict\", sql_type.FloatType()),\n",
    "    StructField(\"r2_test\", sql_type.FloatType()),\n",
    "    StructField(\"mse\", sql_type.FloatType()),\n",
    "    StructField(\"mape\", sql_type.FloatType()),\n",
    "    StructField(\"model_type\", sql_type.StringType()),\n",
    "    StructField(\"model\", sql_type.StringType()),\n",
    "    StructField(\"feature_importance\", sql_type.StringType()),\n",
    "    StructField(\"valid_flag\", sql_type.FloatType()),\n",
    "    StructField(\"target_qtty\", sql_type.FloatType())\n",
    "])\n",
    "\n",
    "\n",
    "def format_result_sku(row):\n",
    "    return (\n",
    "        str(row['dt']),\n",
    "        str(row['item_sku_id']),\n",
    "        float(row['target_price']),\n",
    "        float(row['prediction']),\n",
    "        float(row['r2_predict']),\n",
    "        float(row['r2_test']),\n",
    "        float(row['mse']),\n",
    "        float(row['mape']),\n",
    "        str(row['model_type']),\n",
    "        str(row['model']),\n",
    "        str(row['feature_importance']),\n",
    "        float(row['valid_flag']),\n",
    "        float(row['target_qtty'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_pandas_3 = spark.createDataFrame(df_1_pandas_2)\n",
    "df_1_pandas_3.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update_predict = ['lr', 'hr', 'rf', 'prophet']\n",
    "# 使用4种模型\n",
    "update_predict = ['lr']\n",
    "\n",
    "# 按照dt打标，valid_flag test_flag\n",
    "df_dt = df_1_pandas_3.select('item_sku_id','dt').distinct()\n",
    "df_dt = df_dt.withColumn('rank',F.row_number().over(Window.partitionBy('item_sku_id').orderBy(F.col('dt').desc())))\\\n",
    "             .withColumn('valid_flag',F.when(F.col('rank')<=30,1)\\\n",
    "                                       .when((F.col('rank')<=45)&(F.col('rank')>=30),2).otherwise(F.lit(0)))\n",
    "\n",
    "df_lu = df_1_pandas_3.join(df_dt.select('item_sku_id','dt','valid_flag'),['item_sku_id','dt'],'inner')\n",
    "\n",
    "# df_sku_count = df_lu.filter(F.col('valid_flag').isin([0,4,5])).groupBy('item_sku_id')\\\n",
    "df_sku_count = df_lu.filter(F.col('valid_flag').isin([0])).groupBy('item_sku_id')\\\n",
    "                    .agg(F.count('dt').alias('count'))\\\n",
    "                    .filter(F.col('count')>=30)\n",
    "# 过滤出训练集数据足够多的天\n",
    "df_2 = df_lu.join(df_sku_count.select('item_sku_id'), ['item_sku_id'], 'inner')\n",
    "df_2.cache()\n",
    "df_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------------+-----------+----------+\n",
      "| item_sku_id|        dt|      target_price|target_qtty|valid_flag|\n",
      "+------------+----------+------------------+-----------+----------+\n",
      "|100000022911|2018-10-29| 297.5365853658537|         41|         0|\n",
      "|100000022911|2019-05-12|251.15621621621622|         37|         0|\n",
      "|100000022911|2019-03-16|277.57142857142856|         14|         0|\n",
      "|100000022911|2019-09-27|218.38636363636363|         44|         1|\n",
      "|100000022911|2019-01-18|254.07142857142858|         28|         0|\n",
      "|100000022911|2019-04-25| 248.3776397515528|        161|         0|\n",
      "|100000022911|2018-10-27|297.97297297297297|         37|         0|\n",
      "|100000022911|2019-05-10| 251.0738888888889|         18|         0|\n",
      "|100000022911|2018-10-23|288.04545454545456|         44|         0|\n",
      "|100000022911|2019-05-06|248.78050000000002|         40|         0|\n",
      "|100000022911|2018-12-28|257.05882352941177|         17|         0|\n",
      "|100000022911|2019-07-11| 266.3478260869565|         23|         0|\n",
      "|100000022911|2018-11-14|253.91944444444445|         18|         0|\n",
      "|100000022911|2019-02-19| 238.6206896551724|         87|         0|\n",
      "|100000022911|2019-09-02| 233.9655172413793|         29|         2|\n",
      "|100000022911|2019-03-13| 235.7037037037037|         54|         0|\n",
      "|100000022911|2019-09-24|237.15217391304347|         46|         1|\n",
      "|100000022911|2018-12-03|266.85714285714283|         14|         0|\n",
      "|100000022911|2019-06-16| 197.6434146341464|        123|         0|\n",
      "|100000022911|2018-11-03|245.66842105263157|         38|         0|\n",
      "+------------+----------+------------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPA_simulation_functions import *\n",
    "spark.sparkContext.addPyFile('SPA_simulation_functions.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出验证集结果\n",
    "# 每个sku有多个模型结果\n",
    "# df_2 = df_2.withColumn('test_flag',F.when(F.col('valid_flag').isin([0,4,5]),F.lit(0)).otherwise(F.lit(1)))\n",
    "df_2 = df_2.withColumn('test_flag',F.when(F.col('valid_flag').isin([0]),F.lit(0)).otherwise(F.lit(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = df_2.select(selected_columns).rdd.map(lambda row: ((row['item_sku_id']), row)).groupByKey()\\\n",
    "    .flatMap(lambda row : calculate_baseline_sku(row, update_predict, selected_columns, \n",
    "                                                 X_SCHEMA_SKU, rolling_columns, 'self'))\n",
    "\n",
    "result_df2 = spark.createDataFrame(result2.map(format_result_sku), schema=SCHEMA_OUTPUT_SKU)\n",
    "\n",
    "# result_df2 = result_df2.na.drop(subset=['prediction'])\n",
    "# result_df2.cache()\n",
    "# result_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df2.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
