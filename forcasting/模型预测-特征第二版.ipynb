{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time \n",
    "from pyspark.sql.types import StructType \n",
    "from pyspark.sql.types import StructField\n",
    "import pyspark.sql.types as sql_type\n",
    "name = locals()\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/bin/python3\"\n",
    "\n",
    "spark = (SparkSession\\\n",
    "         .builder\\\n",
    "         .appName(\"test\")\\\n",
    "         .enableHiveSupport()\\\n",
    "         .config(\"spark.executor.instances\", \"200\")\\\n",
    "         .config(\"spark.executor.memory\",\"16g\")\\\n",
    "         .config(\"spark.executor.cores\",\"4\")\\\n",
    "         .config(\"spark.driver.memory\",\"40g\")\\\n",
    "         .config(\"spark.sql.shuffle.partitions\",\"800\")\\\n",
    "         .config(\"spark.default.parallelism\",\"800\")\\\n",
    "         .config(\"spark.driver.maxResultSize\", \"8g\")\\\n",
    "         .config(\"spark.pyspark.python\", \"/usr/bin/python3\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "import spa_utils\n",
    "\n",
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions=200551''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions.pernode=200551''')\n",
    "params = {'author':'xiaoxiao10'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_dt = '2019-09-13'\n",
    "sku_list = spark.sql('''select \n",
    "sku_id as item_sku_id,\n",
    "cid3\n",
    "from dev.self_sku_det_da\n",
    "where dt='%s'\n",
    "and sku_type = 1\n",
    "and cid3 in('740','2676') '''%latest_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非日期特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling 销量特征\n",
    "features_from_luyuan =  spark.sql('''select * from app.app_pa_simulation_feature_evaluation_2019_06_30_self''')\n",
    "features_qtty_rolling = features_from_luyuan.select(['item_sku_id','dt','netprice','sale_qtty','rolling360mean', 'rolling180mean', 'rolling90mean','rolling28mean',\\\n",
    "                                                     'rolling14mean','rolling7mean','rolling5mean','rolling3mean',\\\n",
    "                                                    'rolling2mean','rolling1mean','rolling14median','rolling7median',\\\n",
    "                                                    'rolling360decaymean','rolling180decaymean','rolling90decaymean','rolling28decaymean',\\\n",
    "                                                    'rolling14decaymean','rolling7decaymean','rolling3decaymean','decomposedtrend'])\n",
    "features_qtty_rolling = features_qtty_rolling.join(sku_list,'item_sku_id','inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_qtty_rolling_2 = features_qtty_rolling.withColumn('gmv', F.col('netprice')*F.col('sale_qtty')).groupBy('cid3','dt').agg(F.sum('gmv').alias('cid3_gmv'))\n",
    "\n",
    "features_qtty_rolling_2_pd = features_qtty_rolling_2.toPandas()\n",
    "\n",
    "features_qtty_rolling_3 = features_qtty_rolling_2_pd.sort_values(['cid3','dt']).set_index('dt')\n",
    "\n",
    "features_qtty_rolling_4 = features_qtty_rolling_3.groupby('cid3').apply(lambda x: pd.concat([seasonal_decompose(x[['cid3_gmv']], freq=365, model='additive', two_sided=False).trend.rename(columns={'cid3_gmv':'cid3trend'})\\\n",
    "                                                         ,seasonal_decompose(x[['cid3_gmv']], freq=365, model='additive', two_sided=False).seasonal.rename(columns={'cid3_gmv':'cid3seasonal'})],axis=1))\n",
    "\n",
    "features_qtty_rolling_5 = spark.createDataFrame(features_qtty_rolling_4.reset_index().fillna(0))\n",
    "features_qtty_rolling_6 = features_qtty_rolling.join(features_qtty_rolling_5,['cid3','dt'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling uv特征\n",
    "df_uv = spark.sql('''select sku_id as item_sku_id, uv, dt from dev.all_sku_traffic''')\n",
    "df_sku_list_uv = sku_list.join(df_uv, ['item_sku_id'], 'inner').filter(F.col('uv').isNotNull())\n",
    "# rolling 1\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-1,-1)\n",
    "df_sku_list_uv = df_sku_list_uv.withColumn('uv_1',F.mean('uv').over(window))\n",
    "# rolling 3\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-3,-1)\n",
    "df_sku_list_uv = df_sku_list_uv.withColumn('uv_3',F.mean('uv').over(window))\n",
    "# rolling 7\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-7,-1)\n",
    "df_sku_list_uv = df_sku_list_uv.withColumn('uv_7',F.mean('uv').over(window))\n",
    "# rolling 15 \n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-15,-1)\n",
    "df_sku_list_uv = df_sku_list_uv.withColumn('uv_15',F.mean('uv').over(window))\n",
    "# rolling 30\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-30,-1)\n",
    "df_sku_list_uv = df_sku_list_uv.withColumn('uv_30',F.mean('uv').over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling stock_qtty & stock_status 特征\n",
    "df_stock = spark.sql('''select sku_id as item_sku_id,dt,stock_status,stock_qtty from dev.dp_pl_es_ext_v2''')\n",
    "df_sku_list_stock = sku_list.join(df_stock, ['item_sku_id'], 'inner').filter(F.col('stock_qtty').isNotNull())\n",
    "# rolling 1\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-1,-1)\n",
    "df_sku_list_stock = df_sku_list_stock.withColumn('stock_qtty_1',F.mean('stock_qtty').over(window)).withColumn('stock_status_1',F.mean('stock_status').over(window))\n",
    "# rolling 3\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-3,-1)\n",
    "df_sku_list_stock = df_sku_list_stock.withColumn('stock_qtty_3',F.mean('stock_qtty').over(window)).withColumn('stock_status_3',F.mean('stock_status').over(window))\n",
    "# rolling 7\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-7,-1)\n",
    "df_sku_list_stock = df_sku_list_stock.withColumn('stock_qtty_7',F.mean('stock_qtty').over(window)).withColumn('stock_status_7',F.mean('stock_status').over(window))\n",
    "# rolling 15 \n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-15,-1)\n",
    "df_sku_list_stock = df_sku_list_stock.withColumn('stock_qtty_15',F.mean('stock_qtty').over(window)).withColumn('stock_status_15',F.mean('stock_status').over(window))\n",
    "# rolling 30\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-30,-1)\n",
    "df_sku_list_stock = df_sku_list_stock.withColumn('stock_qtty_30',F.mean('stock_qtty').over(window)).withColumn('stock_status_30',F.mean('stock_status').over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling red_price\n",
    "df_redprice = spark.sql('''select sku_id as item_sku_id, max_price as redprice, dt from %s ''' % ('dev.self_sku_redprice_group'))\n",
    "df_sku_list_redprice = sku_list.join(df_redprice, ['item_sku_id'], 'inner').filter(F.col('redprice').isNotNull())\n",
    "# rolling 1\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-1,-1)\n",
    "df_sku_list_redprice = df_sku_list_redprice.withColumn('redprice_1',F.mean('redprice').over(window))\n",
    "# rolling 3\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-3,-1)\n",
    "df_sku_list_redprice = df_sku_list_redprice.withColumn('redprice_3',F.mean('redprice').over(window))\n",
    "# rolling 7\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-7,-1)\n",
    "df_sku_list_redprice = df_sku_list_redprice.withColumn('redprice_7',F.mean('redprice').over(window))\n",
    "# rolling 15 \n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-15,-1)\n",
    "df_sku_list_redprice = df_sku_list_redprice.withColumn('redprice_15',F.mean('redprice').over(window))\n",
    "# rolling 30\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-30,-1)\n",
    "df_sku_list_redprice = df_sku_list_redprice.withColumn('redprice_30',F.mean('redprice').over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling base_price & deal_price\n",
    "df_price = spark.sql('''select item_sku_id, price as dealprice, baseprice, dt, base_qtty from app.app_pa_baseline_baseprice_60_xiaoxiao_21''')\n",
    "df_sku_list_price = sku_list.join(df_price, ['item_sku_id'], 'inner').filter(F.col('baseprice').isNotNull()).filter(F.col('dealprice').isNotNull())\n",
    "# rolling 1 \n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-1,-1)\n",
    "df_sku_list_price = df_sku_list_price.withColumn('baseprice_deal_1',F.mean('baseprice').over(window)).withColumn('dealprice_1',F.mean('dealprice').over(window))\n",
    "# rolling 3\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-3,-1)\n",
    "df_sku_list_price = df_sku_list_price.withColumn('baseprice_deal_3',F.mean('baseprice').over(window)).withColumn('dealprice_3',F.mean('dealprice').over(window))\n",
    "# rolling 7\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-7,-1)\n",
    "df_sku_list_price = df_sku_list_price.withColumn('baseprice_deal_7',F.mean('baseprice').over(window)).withColumn('dealprice_7',F.mean('dealprice').over(window))\n",
    "# rolling 15 \n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-15,-1)\n",
    "df_sku_list_price = df_sku_list_price.withColumn('baseprice_deal_15',F.mean('baseprice').over(window)).withColumn('dealprice_15',F.mean('dealprice').over(window))\n",
    "# rolling 30\n",
    "window = Window.partitionBy('item_sku_id','cid3').orderBy('dt').rowsBetween(-30,-1)\n",
    "df_sku_list_price = df_sku_list_price.withColumn('baseprice_deal_30',F.mean('baseprice').over(window)).withColumn('dealprice_30',F.mean('dealprice').over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next day red_price = 路远表中的红价（most popular red price in last 30 days）\n",
    "df_baseprice_red = spark.sql('''select item_sku_id, dt, baseprice as baseprice_red from app.app_pa_price_baseprice_self''')\n",
    "df_baseprice_red = sku_list.join(df_baseprice_red, ['item_sku_id'], 'inner').filter(F.col('baseprice_red').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1245968"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = features_qtty_rolling_6.join(df_sku_list_uv,['item_sku_id','cid3','dt'],'inner').join(df_sku_list_stock,['item_sku_id','cid3','dt'],'inner').join(df_sku_list_redprice,['item_sku_id','cid3','dt'],'inner')\\\n",
    ".join(df_sku_list_price,['item_sku_id','cid3','dt'],'inner').join(df_baseprice_red,['item_sku_id','cid3','dt'],'inner')\n",
    "df_raw.cache()\n",
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日期特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = spark.sql('''select * from app.app_pa_time''') 这个不能用，因为只有一天有标识\n",
    "# 端午节取消，因为会属于618\n",
    "# 清明节给0，因为从历史看清明节 gmv没什么特别的\n",
    "# 中秋节也给0，因为从历史看中秋节 gmv也没什么特别的\n",
    "\n",
    "holidays = pd.DataFrame({'2017-01-01':[0.2],'2017-01-27':[-0.2],'2017-01-28':[-0.2],'2017-04-04':[0],'2017-05-01':[0.2],\n",
    "                         '2017-10-04':[0],'2017-06-01':[0.4],'2017-06-18':[0.8],'2017-11-01':[0.5],'2017-11-11':[1],\n",
    "                         '2018-01-01':[0.2],'2019-01-01':[0.2],'2018-02-15':[-0.2],'2018-02-16':[-0.2],'2019-02-04':[-0.2],\n",
    "                         '2019-02-05':[-0.2],'2018-04-05':[0],'2019-04-05':[0],'2018-05-01':[0.2],'2019-05-01':[0.2],'2018-09-22':[0],\n",
    "                         '2019-09-13':[0],'2018-10-01':[0.2],'2019-10-01':[0.2],'2018-06-01':[0.4],'2018-06-18':[0.8],'2019-06-01':[0.4],\n",
    "                         '2019-06-18':[0.8],'2018-11-01':[0.5],'2018-11-11':[1],'2019-11-01':[0.5],'2019-11-11':[1]})\n",
    "holidays = pd.DataFrame(holidays.unstack()).reset_index()[['level_0',0]].rename(columns={'level_0':'dt',0:'days_flag'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def dateRange(start, end, step=1, format=\"%Y-%m-%d\"):\n",
    "    strptime, strftime = datetime.datetime.strptime, datetime.datetime.strftime\n",
    "    days = (strptime(end, format) - strptime(start, format)).days\n",
    "    return [strftime(strptime(start, format) + datetime.timedelta(i), format) for i in range(0, days+1, step)]\n",
    "\n",
    "for day in dateRange('2017-06-02','2017-06-17'):\n",
    "    holidays.loc[len(holidays)] = [day,0.2]\n",
    "for day in dateRange('2017-11-02','2017-11-10'):\n",
    "    holidays.loc[len(holidays)] = [day,0.2] \n",
    "    \n",
    "for day in dateRange('2018-06-02','2018-06-17'):\n",
    "    holidays.loc[len(holidays)] = [day,0.2]\n",
    "for day in dateRange('2018-11-02','2018-11-10'):\n",
    "    holidays.loc[len(holidays)] = [day,0.2]\n",
    "    \n",
    "for day in dateRange('2019-06-02','2019-06-17'):\n",
    "    holidays.loc[len(holidays)] = [day,0.2]\n",
    "for day in dateRange('2019-11-02','2019-11-10'):\n",
    "    holidays.loc[len(holidays)] = [day,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_day = pd.DataFrame({'dt':dateRange('2017-01-01','2019-12-31')})\n",
    "\n",
    "time_feature = spark.createDataFrame(pd.merge(all_day,holidays,on = 'dt',how='left').fillna(0))\n",
    "time_feature.cache()\n",
    "time_feature.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1245968"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = df_raw.join(time_feature,'dt','inner')\n",
    "feature.cache()\n",
    "feature.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取未来的label与未来的某些特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = feature.select('item_sku_id', 'cid3', 'netprice', 'sale_qtty','days_flag','decomposedtrend','cid3trend' ,'dt').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测未来30天的\n",
    "future_days =45\n",
    "# 未来30天中，该价格在1%的误差范围内一共出现了5次\n",
    "price_least_times = 5\n",
    "window = Window.partitionBy('item_sku_id', 'cid3').orderBy('dt').rowsBetween(15,future_days)\n",
    "df_final = df_data.withColumn('dt_price_qtty', F.collect_list(F.array(F.col('netprice'),F.col('sale_qtty'),F.col('decomposedtrend'),F.col('cid3trend'),F.col('days_flag'),F.col('dt'))).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median(data):\n",
    "    data.sort()\n",
    "    half = len(data) // 2\n",
    "    return (data[half] + data[~half]) / 2\n",
    "\n",
    "# 必须是连续的未来30天，不然就截断至30天前的最后一天\n",
    "def truncates(dt, price_list):\n",
    "    if price_list == []:\n",
    "        return([])\n",
    "    else:\n",
    "        for i in range(len(price_list)):\n",
    "            if price_list[i][5] > ((datetime.datetime.strptime(dt, '%Y-%m-%d')) + datetime.timedelta(30)).strftime('%Y-%m-%d'):\n",
    "                return(price_list[:i])\n",
    "        return(price_list)\n",
    "\n",
    "    \n",
    "def price_qtty(dt,price_list):\n",
    "\n",
    "    price_list = truncates(dt,price_list)\n",
    "    if price_list ==[]:\n",
    "        return([])\n",
    "    else:\n",
    "        price_list_sorted = sorted(price_list,reverse = True)\n",
    "        batch_mean_price = []\n",
    "        batch_num_count = []\n",
    "        batch_mean_qtty = []\n",
    "        batch_num_trend = []\n",
    "        batch_num_cid3trend = []\n",
    "        batch_num_days = []        \n",
    "\n",
    "        while price_list_sorted:\n",
    "            batch = []\n",
    "            qtty = []\n",
    "            trend = []\n",
    "            cid3trend = []\n",
    "            days = []\n",
    "            for i in range(len(price_list_sorted)):\n",
    "                if round(float(price_list_sorted[i][0])-0.99*float(price_list_sorted[0][0]),4)>=0:\n",
    "                    batch.append(float(price_list_sorted[i][0]))\n",
    "                    qtty.append(float(price_list_sorted[i][1]))\n",
    "                    trend.append(float(price_list_sorted[i][2]))\n",
    "                    cid3trend.append(float(price_list_sorted[i][3]))\n",
    "                    days.append(float(price_list_sorted[i][4]))\n",
    "                    max_i = i\n",
    "            batch_mean_price.append(sum(batch) / len(batch))\n",
    "            batch_num_count.append(max_i+1)\n",
    "            batch_mean_qtty.append(sum(qtty)/len(qtty))\n",
    "            batch_num_trend.append(sum(trend)/len(trend))\n",
    "            batch_num_cid3trend.append(sum(cid3trend)/len(cid3trend))\n",
    "            batch_num_days.append(sum(days)/len(days))       \n",
    "            price_list_sorted = price_list_sorted[max_i+1:]\n",
    "\n",
    "        batch_mean_copy = batch_mean_price.copy()\n",
    "        medium = get_median(batch_mean_copy)\n",
    "        ks = [[x,y,z,a,b,c] for x,y,z,a,b,c in zip(batch_mean_price,batch_num_count,batch_mean_qtty,batch_num_trend,batch_num_cid3trend,batch_num_days) if x>=medium*0.1]\n",
    "        price_qtty = [x for x in ks if x[1]>=5]\n",
    "        return(price_qtty)\n",
    "\n",
    "\n",
    "price_qtty_udf = F.udf(price_qtty,ArrayType(ArrayType(FloatType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1245968"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_1 = df_final.withColumn('price_qtty', price_qtty_udf(F.col('dt'),F.col('dt_price_qtty')))\n",
    "df_final_1.cache()\n",
    "df_final_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043972"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_2 = df_final_1.withColumn('explode', F.explode('price_qtty'))\\\n",
    ".withColumn('target_price', F.col('explode')[0])\\\n",
    ".withColumn('target_qtty', F.col('explode')[2])\\\n",
    ".withColumn('target_trend', F.col('explode')[3])\\\n",
    ".withColumn('target_cid3trend', F.col('explode')[4])\\\n",
    ".withColumn('target_days_flag', F.col('explode')[5])\n",
    "\n",
    "df_final_2.cache()\n",
    "df_final_2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征+label合并存表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043972"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = feature.join(df_final_2.select('item_sku_id','cid3','dt','target_trend','target_cid3trend','target_days_flag','target_price','target_qtty'),['item_sku_id','cid3','dt'],'inner')\n",
    "model_data.cache()\n",
    "model_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一版的xiaoxiao_3是一直使用的训练数据，时间是到8月份的  \n",
    "现在最新使用的xiaoxiao_6不会用来训练模型，只是单纯用作最后的测试集，但只多刷到9月份，刨去最后45天不能用，其实也没有多少测试数据  \n",
    "未来可以再刷xiaoxiao_6以得到更多的测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_data2 = model_data.withColumn('dt1',F.col('dt')).drop('dt').withColumnRenamed('dt1','dt')\n",
    "\n",
    "spa_utils.save_hive_result(model_data2,'dev.dev_xianzhi_model_data_xiaoxiao_6',partitioning_columns=['dt'],write_mode='save',spark=spark,params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
