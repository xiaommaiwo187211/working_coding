{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import os\n",
    "app_name = 'selection bu batch100'\n",
    "# spark = SparkSession.builder.appName(app_name).enableHiveSupport().getOrCreate()\n",
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/bin/python3\"\n",
    "\n",
    "spark = (SparkSession\\\n",
    "         .builder\\\n",
    "         .appName(\"test\")\\\n",
    "         .enableHiveSupport()\\\n",
    "         .config(\"spark.executor.instances\", \"100\")\\\n",
    "         .config(\"spark.executor.memory\",\"48g\")\\\n",
    "         .config(\"spark.executor.cores\",\"4\")\\\n",
    "         .config(\"spark.driver.memory\",\"40g\")\\\n",
    "         .config(\"spark.sql.shuffle.partitions\",\"2000\")\\\n",
    "         .config(\"spark.default.parallelism\",\"2000\")\\\n",
    "         .config(\"spark.driver.maxResultSize\", \"8g\")\\\n",
    "         .config(\"spark.pyspark.python\", \"/usr/bin/python3\")\\\n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "import pyspark.sql.types as sql_type\n",
    "import spa_utils\n",
    "\n",
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions=200551''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions.pernode=200551''')\n",
    "params = {'author':'xiaoxiao10'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error,f1_score,precision_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.spatial.distance import cosine\n",
    "name = locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191383"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_result3 = spark.sql('''select * from dev.dev_xianzhi_model_data_3_result_zou ''')\n",
    "second_result3.cache()\n",
    "second_result3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读需要预测的数据及特征(新数据剔除旧数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043972"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql('''select * from dev.dev_xianzhi_model_data_xiaoxiao_6''')\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用来预测的预测集必须在45天之前，才能保证数据完整\n",
    "df_dt = df.select('item_sku_id','dt').distinct()\n",
    "df_dt = df_dt.withColumn('rank',F.row_number().over(Window.partitionBy('item_sku_id').orderBy(F.col('dt').desc())))\n",
    "df_dt = df_dt.withColumn('predict_flag',F.when(F.col('rank')<=45,0).otherwise(F.lit(1)))\n",
    "\n",
    "# 每个sku可以使用模型输出数据集的倒数15天之后的数据，这样才数据泄露，而这些数据在之前由于不完整所以未使用\n",
    "# 且模型输出数据集中没有sku，不会参与新的预测\n",
    "# 带着历史最高销量字段\n",
    "model_dt = second_result3.select('item_sku_id','dt','max_qtty').distinct()\n",
    "model_dt = model_dt.withColumn('rank',F.row_number().over(Window.partitionBy('item_sku_id').orderBy(F.col('dt').desc())))\n",
    "model_dt2 = model_dt.filter(F.col('rank')==15).select('item_sku_id','dt','max_qtty').withColumnRenamed('dt','valid_dt')\n",
    "\n",
    "test_dt = df_dt.filter(F.col('predict_flag')==1).select('item_sku_id','dt')\\\n",
    "               .join(model_dt2,'item_sku_id','inner')\n",
    "\n",
    "test_dt2 = test_dt.filter(F.col('dt')>=F.col('valid_dt')).select('item_sku_id','dt','max_qtty')\n",
    "\n",
    "predict_df = df.join(test_dt2,['item_sku_id','dt'],'inner')\n",
    "predict_df.cache()\n",
    "predict_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用存储的模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_record = second_result3.select('item_sku_id','model_type','model','huber_model').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df = predict_df.join(model_record,'item_sku_id','inner')\n",
    "predict_df.cache()\n",
    "predict_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SCHEMA_SKU = ['target_price', 'decomposedtrend','cid3trend','target_days_flag']\n",
    "\n",
    "import pickle\n",
    "import codecs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble  import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "SCHEMA_OUTPUT_SKU = StructType([\n",
    "    StructField(\"item_sku_id\", sql_type.StringType()),\n",
    "    StructField(\"dt\", sql_type.StringType()),\n",
    "    StructField(\"model_type\", sql_type.StringType()),\n",
    "    StructField(\"model\", sql_type.StringType()),\n",
    "    StructField(\"huber_model\", sql_type.StringType()),\n",
    "    StructField(\"target_price\", sql_type.FloatType()),\n",
    "    StructField(\"target_qtty\", sql_type.FloatType()),\n",
    "    StructField(\"predict\", sql_type.FloatType()),\n",
    "    StructField(\"predict2\", sql_type.FloatType())\n",
    "])\n",
    "\n",
    "\n",
    "def format_result_sku(row):\n",
    "    return (\n",
    "        str(row['item_sku_id']),\n",
    "        str(row['dt']),\n",
    "        str(row['model_type']),\n",
    "        str(row['model']),\n",
    "        str(row['huber_model']),\n",
    "        float(row['target_price']),\n",
    "        float(row['target_qtty']),\n",
    "        float(row['predict']),\n",
    "        float(row['predict2'])\n",
    "    )\n",
    "\n",
    "def yichang(ddf):\n",
    "    ddf = ddf.reset_index()\n",
    "    if list(ddf.sort_values('target_price')['index']) != list(ddf.sort_values('target_qtty',ascending=False)['index']):\n",
    "        return 1\n",
    "    elif list(ddf.sort_values('target_price')['index']) != list(ddf.sort_values('predict',ascending=False)['index']):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def smooth(row):    \n",
    "    df = pd.DataFrame(list(row[1]),columns=['item_sku_id','dt']+X_SCHEMA_SKU+['model_type','model','huber_model','max_qtty','target_qtty'])\n",
    "    \n",
    "    # 正常模型预测输出\n",
    "    sku_model = pickle.loads(codecs.decode(df.iloc[0]['model'].encode(), 'base64'))\n",
    "    if df.iloc[0]['model_type'] == 'xgboost':\n",
    "        x = xgboost.DMatrix(df[X_SCHEMA_SKU].values)\n",
    "        predict_qtty = np.exp(sku_model.predict(x))-1\n",
    "        df2 = pd.concat([df,pd.DataFrame({'predict':predict_qtty})],axis=1)\n",
    "        \n",
    "    elif df.iloc[0]['model_type'] == 'prophet':\n",
    "        df['ds'] = pd.to_datetime(df['dt'])\n",
    "        x = df[['ds'] + ['target_price', 'target_days_flag']].reset_index(drop=True)\n",
    "        forecast = sku_model.predict(x)\n",
    "        df2 = df.copy()\n",
    "        df2['predict'] = forecast['yhat']        \n",
    "    else:\n",
    "        x = df[X_SCHEMA_SKU]\n",
    "        predict_qtty = np.exp(sku_model.predict(x))-1\n",
    "        df2 = pd.concat([df,pd.DataFrame({'predict':predict_qtty})],axis=1)\n",
    "\n",
    "    \n",
    "    # 一天有多个价格的随机森林模型，如果发生异常顺序，使用huber再预测一次\n",
    "    yichang_dt = df2.groupby('dt').apply(lambda x:yichang(x)).reset_index().rename(columns={0:'dt_flag'})\n",
    "    yichang_dt = yichang_dt.loc[yichang_dt['dt_flag']==1,'dt']\n",
    "    \n",
    "    \n",
    "    if (df.iloc[0]['model_type'] == 'rf') & (len(yichang_dt)>0):\n",
    "        huber_model = pickle.loads(codecs.decode(df.iloc[0]['huber_model'].encode(), 'base64'))\n",
    "        \n",
    "        predict_qtty2 = huber_model.predict(df2[['target_price']])\n",
    "        df3 = pd.concat([df2,pd.DataFrame({'predict2':predict_qtty2})],axis=1)\n",
    "        df3.loc[df3['dt'].isin(list(yichang_dt))==False,'predict2'] = df3.loc[df3['dt'].isin(list(yichang_dt))==False,'predict']\n",
    "    else:\n",
    "        df3 = df2.copy()\n",
    "        df3['predict2'] = df3['predict']\n",
    "\n",
    "    \n",
    "    # 销量要小于等于历史最大销量，且大于等于0\n",
    "    df3.loc[df3['predict2']>df.iloc[0]['max_qtty'],'predict2'] = df.iloc[0]['max_qtty']\n",
    "    df3.loc[df3['predict2']<0,'predict2'] = 0\n",
    "    \n",
    "    return df3[['item_sku_id','dt','model_type','model','huber_model','target_price','target_qtty','predict','predict2']].to_dict(orient= 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o318.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 26 (count at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to BJLFRZ-10k-186-232.hadoop.jd.local/172.21.186.232:7337 \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:485) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) \tat org.apache.spark.scheduler.Task.run(Task.scala:122) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1363) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Failed to connect to BJLFRZ-10k-186-232.hadoop.jd.local/172.21.186.232:7337 \tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245) \tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187) \tat org.apache.spark.network.shuffle.ExternalShuffleClient.lambda$fetchBlocks$0(ExternalShuffleClient.java:100) \tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141) \tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169) \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) \tat java.util.concurrent.FutureTask.run(FutureTask.java:266) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) \t... 1 more Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: BJLFRZ-10k-186-232.hadoop.jd.local/172.21.186.232:7337 \tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) \tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) \tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323) \tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) \tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) \t... 2 more Caused by: java.net.ConnectException: Connection refused \t... 11 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1904)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1892)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1891)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1508)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2074)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2063)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2150)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2169)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:951)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:369)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:950)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:320)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6c3ad3762693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_result_sku\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCHEMA_OUTPUT_SKU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresult2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresult2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/software/servers/10k/mart_scr/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \"\"\"\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/servers/10k/mart_scr/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/servers/10k/mart_scr/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/servers/10k/mart_scr/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o318.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 26 (count at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to BJLFRZ-10k-186-232.hadoop.jd.local/172.21.186.232:7337 \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:554) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:485) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:64) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_0$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409) \tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) \tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) \tat org.apache.spark.scheduler.Task.run(Task.scala:122) \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1363) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Failed to connect to BJLFRZ-10k-186-232.hadoop.jd.local/172.21.186.232:7337 \tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245) \tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187) \tat org.apache.spark.network.shuffle.ExternalShuffleClient.lambda$fetchBlocks$0(ExternalShuffleClient.java:100) \tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141) \tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169) \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) \tat java.util.concurrent.FutureTask.run(FutureTask.java:266) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) \t... 1 more Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: BJLFRZ-10k-186-232.hadoop.jd.local/172.21.186.232:7337 \tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) \tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) \tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323) \tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) \tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) \t... 2 more Caused by: java.net.ConnectException: Connection refused \t... 11 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1904)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1892)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1891)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1508)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2122)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2074)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2063)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2150)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2169)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:951)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:369)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:950)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:320)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "result = predict_df.select(['item_sku_id','dt']+X_SCHEMA_SKU+['model_type','model','huber_model','max_qtty','target_qtty'])\\\n",
    "                   .rdd.map(lambda row: ((row['item_sku_id']), row)).groupByKey()\\\n",
    "                   .flatMap(lambda row : smooth(row))\n",
    "\n",
    "result2 = spark.createDataFrame(result.map(format_result_sku), schema=SCHEMA_OUTPUT_SKU)\n",
    "result2.cache()\n",
    "result2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result2.withColumn('percent',F.abs(F.col('target_qtty')-F.col('predict'))/F.when(F.col('target_qtty')==0,F.lit(1)).otherwise(F.col('target_qtty')))\\\n",
    "                 .withColumn('second_percent',F.abs(F.col('target_qtty')-F.col('predict2'))/F.when(F.col('target_qtty')==0,F.lit(1)).otherwise(F.col('target_qtty')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.write.mode('overwrite').format('orc').saveAsTable('dev.dev_xianzhi_model_data_3_result_zou_test')\n",
    "spark.sql('''ALTER TABLE dev.dev_xianzhi_model_data_3_result_zou_test set TBLPROPERTIES ('author'='kangwenjia1') ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.agg(F.mean('second_percent')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.filter(F.col('target_qtty')>=10).agg(F.mean('second_percent')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmape = result2.withColumn('cha',F.abs(F.col('target_qtty')-F.col('predict2')))\n",
    "sum_qtty = wmape.filter(F.col('target_qtty')>=10)\\\n",
    "                .agg(F.sum('target_qtty')).collect()[0][0]\n",
    "\n",
    "wmape.filter(F.col('target_qtty')>=10)\\\n",
    "     .agg((F.sum('cha')/sum_qtty).alias('wmape')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
