{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import os\n",
    "app_name = 'selection bu batch100'\n",
    "# spark = SparkSession.builder.appName(app_name).enableHiveSupport().getOrCreate()\n",
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/bin/python3\"\n",
    "\n",
    "spark = (SparkSession\\\n",
    "         .builder\\\n",
    "         .appName(\"test\")\\\n",
    "         .enableHiveSupport()\\\n",
    "         .config(\"spark.executor.instances\", \"150\") \\\n",
    "         .config(\"spark.executor.memory\", \"32g\") \\\n",
    "         .config(\"spark.executor.cores\", \"2\") \\\n",
    "         .config(\"spark.driver.memory\", \"32g\") \\\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8000\") \\\n",
    "         .config(\"spark.default.parallelism\", \"8000\") \\\n",
    "         .config(\"spark.driver.maxResultSize\", \"32g\") \\\n",
    "         .config(\"spark.pyspark.python\", \"/usr/bin/python3\")\\\n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "import pyspark.sql.types as sql_type\n",
    "import spa_utils\n",
    "name = locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定更新的日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每天跑前一天的\n",
    "# 每个时间都可以改，update_start和update_end是需要插入的日期，update_origin必须为update_start的前三个月的日期\n",
    "import datetime\n",
    "import time\n",
    "yesterday = (datetime.datetime.now()-datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "start_day = (datetime.datetime.strptime(spark.sql('''select max(dt) from app.app_pa_price_baseprice_self_deal_price_60_5 ''').collect()[0][0],'%Y-%m-%d')+datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "origin_day = (datetime.datetime.strptime(start_day,'%Y-%m-%d')-91*datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "params = {\n",
    "    'update_origin':origin_day,\n",
    "    'update_start':start_day,\n",
    "    'update_end':yesterday\n",
    "}\n",
    "update_origin = datetime.datetime.strptime(params['update_origin'], '%Y-%m-%d')\n",
    "update_start = params['update_start']\n",
    "update_end = datetime.datetime.strptime(params['update_end'], '%Y-%m-%d')\n",
    "\n",
    "percent = 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60天-红价填充空值再算基线价--去掉双十一618-改成固定的60*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median(data):\n",
    "    data.sort()\n",
    "    half = len(data) // 2\n",
    "    return (data[half] + data[~half]) / 2\n",
    "\n",
    "def base_price(price_list):\n",
    "    price_list_sorted = sorted(price_list,reverse = True)\n",
    "    batch_mean_price = []\n",
    "    batch_num_count = []\n",
    "    base_price =999999999999\n",
    "    while price_list_sorted:\n",
    "        batch = []\n",
    "        for i in range(len(price_list_sorted)):\n",
    "            if round(price_list_sorted[i]-0.97*price_list_sorted[0],4)>=0:\n",
    "                batch.append(price_list_sorted[i])\n",
    "                max_i = i\n",
    "#                 print('batch:%s'%batch)\n",
    "#                 print('max_i%s'%max_i)\n",
    "        batch_mean_price.append(sum(batch) / len(batch))\n",
    "        batch_num_count.append(max_i+1)\n",
    "        price_list_sorted = price_list_sorted[max_i+1:]\n",
    "#         print('batch_mean_price%s'%batch_mean_price)\n",
    "#         print('batch_num_count%s'%batch_num_count)  \n",
    "#         print('price_list_sorted%s'%price_list_sorted)  \n",
    "    print(batch_mean_price,batch_num_count)\n",
    "    sum_batch_num_count =  sum(batch_num_count)\n",
    "#    print('sum_batch_num_count%s'%sum_batch_num_count) \n",
    "    batch_mean_copy = batch_mean_price.copy()\n",
    "    medium = get_median(batch_mean_copy)\n",
    "    ks = [[x,y] for x,y in zip(batch_mean_price, batch_num_count) if x>=medium*0.1]\n",
    "#     batch_mean_price = list(filter(lambda x: x>medium*0.1,batch_mean_price))      \n",
    "#     batch_num_count = list(filter(lambda x: x>medium*0.1,batch_num_count))\n",
    "    batch_mean_price = [x[0] for x in ks]\n",
    "    batch_num_count = [x[1] for x in ks]\n",
    "#     print('batch_mean_price:%s'%batch_mean_price)\n",
    "#     print('batch_num_count:%s'%batch_num_count)\n",
    "    for i in range(len(batch_num_count)):\n",
    "        if batch_num_count[i] > 0.3 * 60 and batch_mean_price[i] < base_price:\n",
    "            base_price = batch_mean_price[i]\n",
    "            print('base_price_0%s'%base_price)      \n",
    "    if base_price == 999999999999:\n",
    "        for i in range(len(batch_num_count)):\n",
    "            if batch_num_count[i] == max(batch_num_count) and batch_mean_price[i] < base_price:\n",
    "                base_price = batch_mean_price[i]\n",
    "#         print('a_index%s'%a.index(max(a)))\n",
    "#         print('base_price_1%s'%base_price)      \n",
    "    return(base_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 红价填充成交价没有的天数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269313682"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取成交价与红价，已经处理为(sku, date)粒度，包含全部日期\n",
    "deal = spark.sql('''\n",
    "select item_sku_id as item_sku_id, after_prefr_amount,dq_pay_amount,sale_qtty, dt from %s where dt>='%s' and dt<='%s'\n",
    "''' % ('app.app_pa_transactions_d_self', params['update_origin'],params['update_end']))\n",
    "deal = deal.groupby('item_sku_id','dt').agg(((F.sum('after_prefr_amount')-F.sum('dq_pay_amount'))/F.sum('sale_qtty')).alias('price'),\n",
    "                                        F.sum('sale_qtty').alias('sale_qtty'))\n",
    "deal = deal.withColumn('price',F.when(F.col('price')<0,F.round(F.col('price'),0)).otherwise(F.col('price')))\\\n",
    "       .filter(F.col('price')>=0)\n",
    "\n",
    "redprice = spark.sql('''\n",
    "select sku_id as item_sku_id, max_price as scrapedprice, dt from %s where dt>='%s' and dt<='%s'\n",
    "''' % ('dev.self_sku_redprice_group', params['update_origin'],params['update_end']))\n",
    "\n",
    "# 读取sku上下柜情况\n",
    "df_status = spark.sql('''\n",
    "select sku_id as item_sku_id, sku_status_cd, dt from %s where sku_type=1 and dt>='%s' and dt<='%s'\n",
    "''' % ('dev.self_sku_det_da', params['update_origin'],params['update_end'])).distinct()\n",
    "# 只保留当天上柜的sku\n",
    "df_status = df_status.filter(df_status.sku_status_cd == 3001)\n",
    "\n",
    "df = df_status.join(redprice, ['item_sku_id', 'dt'], 'inner').join(deal,['item_sku_id','dt'],'left')\n",
    "df = df.withColumn('price',F.when(F.col('price').isNull(),F.col('scrapedprice')).otherwise(F.col('price')))\\\n",
    "       .select('item_sku_id','dt','price','sale_qtty')\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新新一天的基线价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570777604.8945994\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions=200551''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions.pernode=200551''')\n",
    "table_params = {'author':'zoushuhan'}\n",
    "\n",
    "print(time.time())\n",
    "days = 60\n",
    "base_0 = df.filter(F.col('dt')>=update_start)\\\n",
    "           .select('item_sku_id','dt')\\\n",
    "           .withColumn('dt_past',F.date_sub(F.col('dt'), 0))\n",
    "\n",
    "for ii in range(1,days+1):\n",
    "    name['base_%s'%ii] = df.filter(F.col('dt')>=update_start).select('item_sku_id','dt').withColumn('dt_past',F.date_sub(F.col('dt'), ii))\n",
    "    base_0 = base_0.union(eval('base_%s'%ii))\n",
    "# 去掉双十一和618再去计算基线价\n",
    "base_0 = base_0.filter((F.substring(F.col('dt_past'),6,5)<='05-31')|((F.substring(F.col('dt_past'),6,5)>='06-21')&(F.substring(F.col('dt_past'),6,5)<='10-31'))|\n",
    "                       (F.substring(F.col('dt_past'),6,5)>='11-14'))\n",
    "df1 = base_0.join(df.withColumnRenamed('dt','dt_past').select('item_sku_id','dt_past','price'),['item_sku_id','dt_past'],'inner') \n",
    "df1 = df1.withColumn('price_1',F.col('price').cast('Float'))\n",
    "\n",
    "grouped_df = df1.groupby(\"item_sku_id\",\"dt\").agg(F.collect_list(\"price_1\").alias(\"list_price\"))\n",
    "\n",
    "\n",
    "# grouped_df = grouped_df.withColumn('dt1',F.col('dt')).drop('dt').withColumnRenamed('dt1','dt')\n",
    "# spa_utils.save_hive_result(grouped_df,'app.app_pa_price_baseprice_self_deal_price_temp',partitioning_columns=['dt'],write_mode='save',spark=spark,params=table_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570761236.7333064\n"
     ]
    }
   ],
   "source": [
    "# grouped_df = spark.sql('''select * from app.app_pa_price_baseprice_self_deal_price_temp ''')\n",
    "base_price_udf = F.udf(base_price,FloatType())\n",
    "df2 = grouped_df.withColumn('baseprice', base_price_udf('list_price'))\n",
    "df2 = df2.select('item_sku_id','baseprice','dt')\n",
    "\n",
    "spa_utils.save_hive_result(df2,'app.app_pa_price_baseprice_self_deal_price_60_5',partitioning_columns=['dt'],write_mode='insert',spark=spark,params=table_params)\n",
    "# spark.sql('''drop table if exists app.app_pa_price_baseprice_self_deal_price_temp ''')\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
