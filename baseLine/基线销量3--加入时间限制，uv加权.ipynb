{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import os\n",
    "app_name = 'selection bu batch100'\n",
    "# spark = SparkSession.builder.appName(app_name).enableHiveSupport().getOrCreate()\n",
    "\n",
    "#os.environ['PYSPARK_PYTHON'] = 'python3.5'\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/bin/python3\"\n",
    "\n",
    "spark = (SparkSession \\\n",
    "         .builder \\\n",
    "         .appName(\"spark_test\") \\\n",
    "         .enableHiveSupport() \\\n",
    "         .config(\"spark.executor.instances\", \"100\") \\\n",
    "         .config(\"spark.executor.memory\", \"48g\") \\\n",
    "         .config(\"spark.executor.cores\", \"48\") \\\n",
    "         .config(\"spark.driver.memory\", \"48g\") \\\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"200000\") \\\n",
    "         .config(\"spark.default.parallelism\", \"200000\") \\\n",
    "         .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "         .config(\"spark.pyspark.python\", \"/usr/bin/python3\")\\\n",
    "         .config('spark.yarn.driver.memoryOverhead', \"64g\")\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "import pyspark.sql.types as sql_type\n",
    "import spa_utils\n",
    "\n",
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions=200551''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions.pernode=200551''')\n",
    "params = {'author':'xiaoxiao10'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sql('''select * from app.app_pa_price_baseprice_self_nonpromo_flag_60_9 ''')\n",
    "\n",
    "# data = data.filter(F.col('item_sku_id')=='100000197925')\n",
    "data = data.select(['item_sku_id','sale_qtty', 'price', 'baseprice','uv','stock_status' ,'non_promo_flag', 'dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputSchema = StructType([StructField('item_sku_id',sql_type.StringType()),StructField('sale_qtty',sql_type.FloatType()),\n",
    "                           StructField('price',sql_type.FloatType()),StructField('baseprice',sql_type.FloatType()),\n",
    "                           StructField('uv',sql_type.IntegerType()),StructField('stock_status',sql_type.FloatType()),\n",
    "                          StructField('non_promo_flag',sql_type.IntegerType()),StructField('base_qtty',sql_type.FloatType()),\n",
    "                           StructField('dt',sql_type.StringType())])\n",
    "\n",
    "def map_func(dictionary):\n",
    "    return(str(dictionary['item_sku_id']),\n",
    "    float(dictionary['sale_qtty']),\n",
    "    float(dictionary['price']),\n",
    "    float(dictionary['baseprice']),\n",
    "    int(dictionary['uv']),\n",
    "    float(dictionary['stock_status']),\n",
    "    int(dictionary['non_promo_flag']),\n",
    "    float(dictionary['base_qtty']),\n",
    "    str(dictionary['dt']))\n",
    "\n",
    "def cal(x,df):\n",
    "    rate = 0.03\n",
    "    x_series = x.iloc[0]\n",
    "    df_1 = df.loc[(abs(df['price']-x_series['baseprice'])<rate*x_series['baseprice'])&(df['dt']!=x_series['dt'])&(df['stock_status']>=0.15)]\n",
    "    df_1['index_2'] = abs(df_1['index']-x_series['index'])\n",
    "    try:\n",
    "        if (min(df_1['index_2']) > 30):\n",
    "            df_2 = df_1.loc[df_1['index_2']<=60]\n",
    "        else:\n",
    "            df_2 = df_1.loc[df_1['index_2']<=30]        \n",
    "\n",
    "        df_2_index = list(df_2.sort_values(['index_2','index']).head(6)['index'])\n",
    "\n",
    "        a = df_2.loc[df_2['index'].isin(df_2_index)]\n",
    "        a['weight'] = np.log(abs(a['uv']-x_series['uv'])+1)\n",
    "        a['weight'] = a['weight'].map(lambda x:0.3 if x==0 else x)\n",
    "        a['weight'] = 1/a['weight']\n",
    "        smooth_qtty_1 = sum(a['weight']*a['sale_qtty'])/sum(a['weight'])      \n",
    "    except:\n",
    "        smooth_qtty_1 = -9999\n",
    "    x['base_qtty'] = smooth_qtty_1\n",
    "    return x\n",
    "\n",
    "def smooth(row):    \n",
    "    df= pd.DataFrame(list(row[1]),columns=['item_sku_id','sale_qtty','price','baseprice','uv','stock_status','non_promo_flag', 'dt'])\n",
    "    df = df.sort_values('dt').reset_index(drop=True)\n",
    "    df['index'] = df['dt'].map(lambda x:(datetime.datetime.strptime(x,'%Y-%m-%d')-datetime.datetime.strptime(min(df['dt']),'%Y-%m-%d')).days)\n",
    "    df['base_qtty'] = -9999\n",
    "    \n",
    "    result = df.groupby('dt').apply(lambda x:cal(x,df))\n",
    "    result = result[['item_sku_id','sale_qtty','price','baseprice','uv','stock_status','non_promo_flag','base_qtty','dt']]\n",
    "    \n",
    "    return result.to_dict(orient= 'records')\n",
    "\n",
    "spark.sql('''set hive.exec.dynamic.partition=true''')\n",
    "spark.sql('''set hive.exec.dynamic.partition.mode=nostrict''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions=200551''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions.pernode=200551''')\n",
    "spark.sql('''set hive.exec.max.created.files=20055100''')\n",
    "spark.sql('''set mapreduce.job.split.metainfo.maxsize=-1''')\n",
    "spark.sql('''set yarn.app.mapreduce.am.command-opts=-Xmx3072m''')\n",
    "spark.sql('''set yarn.app.mapreduce.am.resource.mb=4096''')\n",
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 插入新的日期的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_day = spark.sql('''select max(dt) from app.app_pa_baseline_baseprice_60_xiaoxiao_21''').collect()[0][0]\n",
    "\n",
    "df_1 = data.rdd.map(lambda row: (row['item_sku_id'],row)).groupByKey().flatMap(lambda row: smooth(row))\n",
    "df_final = spark.createDataFrame(df_1.map(map_func),schema=inputSchema)\n",
    "\n",
    "df_final = df_final.select(['item_sku_id', 'sale_qtty', 'price', 'baseprice','uv','stock_status', 'non_promo_flag','base_qtty', 'dt'])\\\n",
    "                   .filter(F.col('dt')>origin_day)\n",
    "spa_utils.save_hive_result(df_final,'app.app_pa_baseline_baseprice_60_xiaoxiao_21',partitioning_columns=['dt'],write_mode='insert',spark=spark,params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仅show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862268631"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = spark.sql('''select * from app.app_pa_baseline_baseprice_60_xiaoxiao_21''')\n",
    "df_3 = df_2.filter(F.col('base_qtty')!=-9999)\n",
    "\n",
    "df_3.cache()\n",
    "df_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|true_flag|    count|\n",
      "+---------+---------+\n",
      "|     true|227703700|\n",
      "|    false| 82797464|\n",
      "|non_equal|221157391|\n",
      "|    equal|330610076|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3 = df_3.withColumn('true_flag',F.when(((F.col('price')>F.col('baseprice'))&(F.col('sale_qtty')<F.col('base_qtty')))|\n",
    "                                          ((F.col('price')<F.col('baseprice'))&(F.col('sale_qtty')>F.col('base_qtty'))),F.lit('true'))\\\n",
    "                                    .when(((F.col('price')>F.col('baseprice'))&(F.col('sale_qtty')>F.col('base_qtty')))|\n",
    "                                          ((F.col('price')<F.col('baseprice'))&(F.col('sale_qtty')<F.col('base_qtty'))),F.lit('false'))\\\n",
    "                                    .when(((F.col('price')==F.col('baseprice'))&(F.col('sale_qtty')!=F.col('base_qtty')))|\n",
    "                                          ((F.col('price')!=F.col('baseprice'))&(F.col('sale_qtty')==F.col('base_qtty'))),F.lit('non_equal'))\\\n",
    "                                    .when((F.col('price')==F.col('baseprice'))&(F.col('sale_qtty')==F.col('base_qtty')),F.lit('equal')))\n",
    "\n",
    "df_3.groupby('true_flag').agg(F.count('dt').alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
