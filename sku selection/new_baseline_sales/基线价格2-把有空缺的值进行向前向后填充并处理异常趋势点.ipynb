{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import os\n",
    "app_name = 'selection bu batch100'\n",
    "# spark = SparkSession.builder.appName(app_name).enableHiveSupport().getOrCreate()\n",
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/bin/python3\"\n",
    "\n",
    "spark = (SparkSession\\\n",
    "         .builder\\\n",
    "         .appName(\"test\")\\\n",
    "         .enableHiveSupport()\\\n",
    "         .config(\"spark.executor.instances\", \"150\") \\\n",
    "         .config(\"spark.executor.memory\", \"32g\") \\\n",
    "         .config(\"spark.executor.cores\", \"4\") \\\n",
    "         .config(\"spark.driver.memory\", \"32g\") \\\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8000\") \\\n",
    "         .config(\"spark.default.parallelism\", \"8000\") \\\n",
    "         .config(\"spark.driver.maxResultSize\", \"32g\") \\\n",
    "         .config(\"spark.pyspark.python\", \"/usr/bin/python3\")\\\n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "import pyspark.sql.types as sql_type\n",
    "import spa_utils\n",
    "name = locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定更新的日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "yesterday = (datetime.datetime.now()-datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "params = {\n",
    "    'update_origin':'2016-10-01',\n",
    "    'update_start':'2017-01-01',\n",
    "    'update_end':yesterday\n",
    "}\n",
    "update_origin = datetime.datetime.strptime(params['update_origin'], '%Y-%m-%d')\n",
    "update_start = params['update_start']\n",
    "update_end = datetime.datetime.strptime(params['update_end'], '%Y-%m-%d')\n",
    "\n",
    "percent = 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取成交价与红价，已经处理为(sku, date)粒度，包含全部日期\n",
    "# 读取sku每天销量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777774738"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal = spark.sql('''\n",
    "select item_sku_id as item_sku_id, after_prefr_amount,dq_pay_amount,sale_qtty, dt from %s where dt>='%s' and dt<='%s'\n",
    "''' % ('app.app_pa_transactions_d_self', params['update_origin'],params['update_end']))\n",
    "deal = deal.groupby('item_sku_id','dt').agg(((F.sum('after_prefr_amount')-F.sum('dq_pay_amount'))/F.sum('sale_qtty')).alias('price'),\n",
    "                                        F.sum('sale_qtty').alias('sale_qtty'))\n",
    "deal = deal.withColumn('price',F.when(F.col('price')<0,F.round(F.col('price'),0)).otherwise(F.col('price')))\\\n",
    "       .filter(F.col('price')>=0)\n",
    "\n",
    "redprice = spark.sql('''\n",
    "select sku_id as item_sku_id, max_price as scrapedprice, dt from %s where dt>='%s' and dt<='%s'\n",
    "''' % ('dev.self_sku_redprice_group', params['update_origin'],params['update_end']))\n",
    "\n",
    "# 读取sku上下柜情况\n",
    "df_status = spark.sql('''\n",
    "select sku_id as item_sku_id, sku_status_cd, dt from %s where sku_type=1 and dt>='%s' and dt<='%s'\n",
    "''' % ('dev.self_sku_det_da', params['update_origin'],params['update_end'])).distinct()\n",
    "# 只保留当天上柜的sku\n",
    "df_status = df_status.filter(df_status.sku_status_cd == 3001)\n",
    "\n",
    "df = df_status.join(redprice, ['item_sku_id', 'dt'], 'inner').join(deal,['item_sku_id','dt'],'left')\n",
    "df = df.withColumn('price',F.when(F.col('price').isNull(),F.col('scrapedprice')).otherwise(F.col('price')))\\\n",
    "       .select('item_sku_id','dt','price','sale_qtty')\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取成交价计算的基线价\n",
    "left join 到上面的df获得每日销量、红价，基线价的表记录数要少于df（红价为基础的表）  \n",
    "会有部分有红价但没有基线价的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_df1 = spark.sql('''select * from app.app_pa_price_baseprice_self_deal_price_60_5 ''')\n",
    "promo_df2 = df.filter(F.col('dt')>=update_start).join(promo_df1,['item_sku_id','dt'],'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 再把处理后的left join到sku上柜表，该表口径是最大的\n",
    "会有上柜但没有红价的情况，更会有上柜有红价但没有基线价的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1782084064"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = df_status.filter(F.col('dt')>=update_start).drop('sku_status_cd').join(promo_df2,['item_sku_id','dt'],'left').fillna(0,subset=['sale_qtty'])\n",
    "c.cache()\n",
    "c.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把空缺的红价和基线价在全部历史数据中先向前填充再向后填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_OUTPUT_SKU = StructType([\n",
    "    StructField(\"item_sku_id\", sql_type.StringType()),\n",
    "    StructField(\"sale_qtty\", sql_type.IntegerType()),\n",
    "    StructField(\"price\", sql_type.FloatType()),\n",
    "    StructField(\"baseprice\", sql_type.FloatType()),\n",
    "#     StructField(\"non_promo_flag\", sql_type.IntegerType()),\n",
    "    StructField(\"dt\", sql_type.StringType())])\n",
    "def format_result_sku(row):\n",
    "    return (\n",
    "        str(row['item_sku_id']),\n",
    "        int(row['sale_qtty']),\n",
    "        float(row['price']),\n",
    "        float(row['baseprice']),\n",
    "#         int(row['non_promo_flag']),\n",
    "        str(row['dt']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def yichang(x,df):\n",
    "#     df['index2'] = abs(df['index'] - xiloc[0]['index'])\n",
    "#     index_list = list(df.sort_values(by=['index2','index'],ascending=[True,True]).head(6)['index'])\n",
    "#     if x['']\n",
    "def calculate(row):\n",
    "    data = pd.DataFrame(list(row[1]), columns=['item_sku_id','dt','price','sale_qtty','baseprice'])\n",
    "    data = data.sort_values('dt',ascending=True).fillna(method='pad').fillna(method='bfill').reset_index()\n",
    "    \n",
    "#     data2 = data.groupby('dt').apply(lambda x:yichang(x,data))    \n",
    "    data2 = data[['item_sku_id','sale_qtty','price','baseprice','dt']].fillna(0) \n",
    "    return data2.to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1782084064"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_result = c.rdd.map(lambda row: ((row['item_sku_id']), row)).groupByKey()\\\n",
    "        .flatMap(lambda row : calculate(row))\n",
    "\n",
    "\n",
    "c2 = spark.createDataFrame(c_result.map(format_result_sku), schema = SCHEMA_OUTPUT_SKU)\n",
    "\n",
    "\n",
    "c2.cache()\n",
    "c2.count()\n",
    "# spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "# spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "# df2.repartition('dt').write.insertInto('app.app_pa_baseline_AR_zou', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去掉异常，用前后三天的平均值代替\n",
    "这里没有只对第4天之后的数据做处理，所以第1天的话就是与2-4天的平均值比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = c2.withColumn('sum_baseprice',F.sum(F.col('baseprice')).over(Window.partitionBy(F.col('item_sku_id')).orderBy(F.col('dt')).rowsBetween(-3,3)))\n",
    "c2 = c2.withColumn('count_baseprice',F.count(F.col('baseprice')).over(Window.partitionBy(F.col('item_sku_id')).orderBy(F.col('dt')).rowsBetween(-3,3)))\n",
    "c2 = c2.withColumn('avg_baseprice',(F.col('sum_baseprice')-F.col('baseprice'))/(F.col('count_baseprice'))-1)\n",
    "\n",
    "c2 = c2.withColumn('baseprice2',F.when((F.col('baseprice')<0.9*F.col('avg_baseprice'))|\n",
    "                                       (F.col('baseprice')>1.1*F.col('avg_baseprice')),F.col('avg_baseprice'))\\\n",
    "                                 .otherwise(F.col('baseprice')))\n",
    "\n",
    "c2 = c2.withColumn('non_promo_flag',F.when(F.round(F.col('price')-F.col('baseprice2'),2)<0,0).otherwise(F.lit(1)))\n",
    "c2_2 = c2.select('item_sku_id','sale_qtty', 'price', 'baseprice2','non_promo_flag', 'dt')\\\n",
    "         .withColumnRenamed('baseprice2','baseprice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# join进计算基线销量所需的流量和库存状态字段并存表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = (datetime.datetime.now()-datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "insert_day = (datetime.datetime.now()-datetime.timedelta(days=4)).strftime('%Y-%m-%d')\n",
    "\n",
    "uv = spark.sql('''SELECT\n",
    "sku_id as item_sku_id,\n",
    "uv,\n",
    "dt\n",
    "from dev.all_sku_traffic\n",
    "where dt >='%s' and dt<='%s'\n",
    "'''%(params['update_start'],params['update_end'])).distinct()\n",
    "\n",
    "# 库存状态\n",
    "stock = spark.sql('''select sku_id as item_sku_id,dt,stock_status from dev.dp_pl_es_ext_v2 where dt >='%s' and dt<='%s'\n",
    "'''%(params['update_start'],params['update_end'])).distinct()\n",
    "\n",
    "origin_day = spark.sql('''select max(dt) from app.app_pa_price_baseprice_self_nonpromo_flag_60_9''').collect()[0][0]\n",
    "\n",
    "c3 = c2_2.join(uv,['item_sku_id','dt'],'left').fillna(0).join(stock,['item_sku_id','dt'],'left').fillna(1)\n",
    "c3 = c3.select('item_sku_id','sale_qtty', 'price', 'baseprice','uv','stock_status' ,'non_promo_flag', 'dt')\\\n",
    "       .filter(F.col('dt')<=insert_day)\\\n",
    "       .filter(F.col('dt')>origin_day)\n",
    "\n",
    "\n",
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions=200551''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions.pernode=200551''')\n",
    "table_params = {'author':'zoushuhan'}\n",
    "spa_utils.save_hive_result(c3,'app.app_pa_price_baseprice_self_nonpromo_flag_60_9',partitioning_columns=['dt'],write_mode='insert',spark=spark,params=table_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
