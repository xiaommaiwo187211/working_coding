{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先用executor.cores=16 启动\n",
    "## 到后面再restart 用executor.cores=4 启动一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "import sys\n",
    "import os\n",
    "app_name = 'selection bu batch100'\n",
    "# spark = SparkSession.builder.appName(app_name).enableHiveSupport().getOrCreate()\n",
    "os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"/usr/bin/python3\"\n",
    "\n",
    "spark = (SparkSession\\\n",
    "         .builder\\\n",
    "         .appName(\"test\")\\\n",
    "         .enableHiveSupport()\\\n",
    "         .config(\"spark.executor.instances\", \"100\")\\\n",
    "         .config(\"spark.executor.memory\",\"48g\")\\\n",
    "         .config(\"spark.executor.cores\",\"4\")\\\n",
    "         .config(\"spark.driver.memory\",\"40g\")\\\n",
    "         .config(\"spark.sql.shuffle.partitions\",\"2000\")\\\n",
    "         .config(\"spark.default.parallelism\",\"2000\")\\\n",
    "         .config(\"spark.driver.maxResultSize\", \"8g\")\\\n",
    "         .config(\"spark.pyspark.python\", \"/usr/bin/python3\")\\\n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.container-executor.class\",\"DockerLinuxContainer\")\\\n",
    "         .config(\"spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .config(\"spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name\",\"bdp-docker.jd.com:5000/wise_mart_rmb_py36:latest\")\\\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as npo\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "import pyspark.sql.types as sql_type\n",
    "import spa_utils\n",
    "\n",
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions=200551''')\n",
    "spark.sql('''set hive.exec.max.dynamic.partitions.pernode=200551''')\n",
    "params = {'author':'xiaoxiao10'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error,f1_score,precision_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.spatial.distance import cosine\n",
    "name = locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043972"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SPA_simulation_functions import *\n",
    "\n",
    "df = spark.sql('''select * from dev.dev_xianzhi_model_data_xiaoxiao_6''')\n",
    "# df = df.withColumn('target_price',F.log(F.col('target_price')))\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile('SPA_simulation_functions.py')\n",
    "\n",
    "selected_columns = ['item_sku_id', 'sale_qtty','cid3' , 'decomposedtrend','cid3trend','rolling360mean', 'rolling180mean',\n",
    "                    'rolling90mean','rolling28mean','rolling14mean','rolling7mean','rolling5mean',\n",
    "                    'rolling3mean','rolling2mean','rolling1mean','rolling14median','rolling7median',\n",
    "                    'rolling360decaymean','rolling180decaymean','rolling90decaymean','rolling28decaymean',\n",
    "                    'rolling14decaymean','rolling7decaymean','rolling3decaymean','uv','uv_1','uv_3','uv_7',\n",
    "                    'uv_15','uv_30','stock_status','stock_qtty','stock_qtty_1','stock_status_1','stock_qtty_3',\n",
    "                    'stock_status_3','stock_qtty_7','stock_status_7','stock_qtty_15','stock_status_15',\n",
    "                    'stock_qtty_30','stock_status_30','redprice','redprice_1','redprice_3','redprice_7',\n",
    "                    'redprice_15','redprice_30','dealprice','baseprice','base_qtty','baseprice_deal_1',\n",
    "                    'dealprice_1','baseprice_deal_3','dealprice_3','baseprice_deal_7','dealprice_7',\n",
    "                    'baseprice_deal_15','dealprice_15','baseprice_deal_30','dealprice_30','baseprice_red',\n",
    "                    'days_flag', 'netprice','target_price','target_trend','target_cid3trend','target_days_flag',\n",
    "                    'test_flag','target_qtty','valid_flag', 'dt']\n",
    "X_SCHEMA_SKU = ['target_price', 'decomposedtrend','cid3trend','target_days_flag']\n",
    "rolling_columns = []\n",
    "SCHEMA_OUTPUT_SKU = StructType([\n",
    "    StructField(\"dt\", sql_type.StringType()),\n",
    "    StructField(\"item_sku_id\", sql_type.StringType()),\n",
    "    StructField(\"target_price\", sql_type.FloatType()),\n",
    "    StructField(\"prediction\", sql_type.FloatType()),\n",
    "    StructField(\"r2_predict\", sql_type.FloatType()),\n",
    "    StructField(\"r2_test\", sql_type.FloatType()),\n",
    "    StructField(\"mse\", sql_type.FloatType()),\n",
    "    StructField(\"mape\", sql_type.FloatType()),\n",
    "    StructField(\"model_type\", sql_type.StringType()),\n",
    "    StructField(\"model\", sql_type.StringType()),\n",
    "    StructField(\"feature_importance\", sql_type.StringType()),\n",
    "    StructField(\"valid_flag\", sql_type.FloatType()),\n",
    "    StructField(\"target_qtty\", sql_type.FloatType())\n",
    "])\n",
    "\n",
    "\n",
    "def format_result_sku(row):\n",
    "    return (\n",
    "        str(row['dt']),\n",
    "        str(row['item_sku_id']),\n",
    "        float(row['target_price']),\n",
    "        float(row['prediction']),\n",
    "        float(row['r2_predict']),\n",
    "        float(row['r2_test']),\n",
    "        float(row['mse']),\n",
    "        float(row['mape']),\n",
    "        str(row['model_type']),\n",
    "        str(row['model']),\n",
    "        str(row['feature_importance']),\n",
    "        float(row['valid_flag']),\n",
    "        float(row['target_qtty'])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992492"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update_predict = ['lr', 'hr', 'rf', 'prophet']\n",
    "# 使用4种模型\n",
    "update_predict = ['lr', 'rf', 'prophet','xgboost']\n",
    "\n",
    "# 按照dt打标，valid_flag test_flag\n",
    "df_dt = df.select('item_sku_id','dt').distinct()\n",
    "df_dt = df_dt.withColumn('rank',F.row_number().over(Window.partitionBy('item_sku_id').orderBy(F.col('dt').desc())))\\\n",
    "             .withColumn('valid_flag',F.when(F.col('rank')<=45,1)\\\n",
    "                                       .when((F.col('rank')<=55)&(F.col('rank')>=46),2)\\\n",
    "                                       .when((F.col('rank')<=85)&(F.col('rank')>=56),3)\\\n",
    "                                       .when((F.col('rank')<=95)&(F.col('rank')>=86),4)\\\n",
    "                                       .when((F.col('rank')<=125)&(F.col('rank')>=96),5).otherwise(F.lit(0)))\n",
    "\n",
    "df_lu = df.join(df_dt.select('item_sku_id','dt','valid_flag'),['item_sku_id','dt'],'inner')\n",
    "\n",
    "# df_sku_count = df_lu.filter(F.col('valid_flag').isin([0,4,5])).groupBy('item_sku_id')\\\n",
    "df_sku_count = df_lu.filter(F.col('valid_flag').isin([0])).groupBy('item_sku_id')\\\n",
    "                    .agg(F.count('dt').alias('count'))\\\n",
    "                    .filter(F.col('count')>=30)\n",
    "\n",
    "df_2 = df_lu.join(df_sku_count.select('item_sku_id'), ['item_sku_id'], 'inner')\n",
    "df_2.cache()\n",
    "df_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "856040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出验证集结果\n",
    "# 每个sku有多个模型结果\n",
    "# df_2 = df_2.withColumn('test_flag',F.when(F.col('valid_flag').isin([0,4,5]),F.lit(0)).otherwise(F.lit(1)))\n",
    "df_2 = df_2.withColumn('test_flag',F.when(F.col('valid_flag').isin([0]),F.lit(0)).otherwise(F.lit(1)))\n",
    "result2 = df_2.select(selected_columns).rdd.map(lambda row: ((row['item_sku_id']), row)).groupByKey()\\\n",
    "    .flatMap(lambda row : calculate_baseline_sku(row, update_predict, selected_columns, \n",
    "                                                 X_SCHEMA_SKU, rolling_columns, 'self'))\n",
    "\n",
    "result_df2 = spark.createDataFrame(result2.map(format_result_sku), schema=SCHEMA_OUTPUT_SKU)\n",
    "\n",
    "result_df2 = result_df2.na.drop(subset=['prediction'])\n",
    "result_df2.cache()\n",
    "result_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选模型\n",
    "result_df2_pd = result_df2.drop('model','feature_importance').toPandas()\n",
    "# result_df3_pd = result_df2_pd.loc[result_df2_pd['valid_flag']==2]\\\n",
    "result_df3_pd = result_df2_pd.loc[result_df2_pd['valid_flag']==4]\\\n",
    "                             .groupby(['item_sku_id','model_type'])\\\n",
    "                             .apply(lambda x:pd.Series([x.iloc[0]['r2_predict'],r2_score(x['target_qtty'],x['prediction'])],index=['train_r2','valid_2_r2']))\\\n",
    "                             .reset_index()\n",
    "\n",
    "result_df4_pd = result_df3_pd.groupby('item_sku_id').apply(lambda x:x.sort_values('valid_2_r2',ascending=False).head(1)).reset_index(drop=True)\n",
    "result_df4 = spark.createDataFrame(result_df4_pd)\n",
    "result_df4.cache()\n",
    "result_df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找历史最高价、最低价、历史最高销量\n",
    "# sale_qtty可以改成target_qtty\n",
    "result_df5 = result_df2.join(result_df4.select(['item_sku_id','model_type']),['item_sku_id','model_type'],'inner')\n",
    "\n",
    "history_price = df_2.filter(F.col('test_flag')==0).groupby('item_sku_id')\\\n",
    "                    .agg(F.min('target_price').alias('min_price'),F.max('target_price').alias('max_price'),F.max('sale_qtty').alias('max_qtty'))\n",
    "\n",
    "# 加入训练集最后一天的特征值\n",
    "last_day_feature = df_2.filter(F.col('test_flag')==0).withColumn('last_day_flag',F.row_number().over(Window.partitionBy('item_sku_id').orderBy(F.col('dt').desc())))\n",
    "last_day_feature = last_day_feature.filter(F.col('last_day_flag')==1).select(list(set(['item_sku_id']+X_SCHEMA_SKU)-set(['target_price'])))\n",
    "\n",
    "second_df = result_df5.join(history_price,['item_sku_id'],'inner')\\\n",
    "                      .join(last_day_feature,['item_sku_id'],'inner')\\\n",
    "                      .select(['item_sku_id','dt','valid_flag','model','model_type','target_qtty','prediction','min_price','max_price','max_qtty']+X_SCHEMA_SKU)\n",
    "# second_df.cache()\n",
    "# second_df.count()\n",
    "\n",
    "second_df.write.mode('overwrite').format('orc').saveAsTable('dev.dev_xianzhi_model_data_6_zou')\n",
    "spark.sql('''ALTER TABLE dev.dev_xianzhi_model_data_6_zou set TBLPROPERTIES ('author'='zoushuhan') ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265866"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_SCHEMA_SKU = ['target_price', 'decomposedtrend','cid3trend','target_days_flag']\n",
    "second_df = spark.sql('''select * from dev.dev_xianzhi_model_data_6_zou ''')\n",
    "\n",
    "second_df.cache()\n",
    "second_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import codecs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble  import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "SCHEMA_OUTPUT_SKU2 = StructType([\n",
    "    StructField(\"item_sku_id\", sql_type.StringType()),\n",
    "    StructField(\"dt\", sql_type.StringType()),\n",
    "    StructField(\"valid_flag\", sql_type.FloatType()),\n",
    "    StructField(\"model_type\", sql_type.StringType()),\n",
    "    StructField(\"model\", sql_type.StringType()),\n",
    "    StructField(\"huber_model\", sql_type.StringType()),\n",
    "    StructField(\"max_qtty\", sql_type.FloatType()),\n",
    "    StructField(\"target_qtty\", sql_type.FloatType()),\n",
    "    StructField(\"target_price\", sql_type.FloatType()),\n",
    "    StructField(\"prediction\", sql_type.FloatType()),\n",
    "    StructField(\"second_prediction\", sql_type.FloatType())\n",
    "])\n",
    "\n",
    "\n",
    "def format_result_sku2(row):\n",
    "    return (\n",
    "        str(row['item_sku_id']),\n",
    "        str(row['dt']),\n",
    "        float(row['valid_flag']),\n",
    "        str(row['model_type']),\n",
    "        str(row['model']),\n",
    "        str(row['huber_model']),\n",
    "        float(row['max_qtty']),\n",
    "        float(row['target_qtty']),\n",
    "        float(row['target_price']),\n",
    "        float(row['prediction']),\n",
    "        float(row['second_prediction'])\n",
    "    )\n",
    "\n",
    "def yichang(ddf):\n",
    "    ddf = ddf.reset_index()\n",
    "    if list(ddf.sort_values('target_price')['index']) != list(ddf.sort_values('target_qtty',ascending=False)['index']):\n",
    "        return 1\n",
    "    elif list(ddf.sort_values('target_price')['index']) != list(ddf.sort_values('prediction',ascending=False)['index']):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def smooth(row):    \n",
    "    df = pd.DataFrame(list(row[1]),columns=['item_sku_id','dt','valid_flag','model','model_type','target_qtty','prediction','min_price','max_price','max_qtty']+X_SCHEMA_SKU)\n",
    "    sku_model = pickle.loads(codecs.decode(df.iloc[0]['model'].encode(), 'base64'))\n",
    "    # 做最大最小间隔，带入原模型，得到预测值\n",
    "    xuni_feature = pd.DataFrame({'target_price':list(range(int(df.iloc[0]['min_price']),int(df.iloc[0]['max_price']+3)))})\n",
    "    # 用训练集最后一天填入其他特征的值\n",
    "    for fea in list(set(X_SCHEMA_SKU)-set(['target_price'])):\n",
    "        xuni_feature[fea] = df.iloc[0][fea]\n",
    "    xuni_y = np.exp(sku_model.predict(xuni_feature[X_SCHEMA_SKU]))-1\n",
    "    xuni_df = pd.concat([xuni_feature,pd.DataFrame({'xuni_y':xuni_y})],axis=1)\n",
    "#     xuni_df['target_price_2'] = xuni_df['target_price']*xuni_df['target_price']\n",
    "    # 建立线性模型\n",
    "    sku_lm_model = HuberRegressor()\n",
    "    sku_model_lm = sku_lm_model.fit(xuni_df[['target_price']],xuni_df[['xuni_y']])\n",
    "    # 带入test的X\n",
    "#     df['target_price_2'] = df['target_price']*df['target_price']   \n",
    "    xuni_ls = sku_model_lm.predict(df[['target_price']])\n",
    "    huber_m = codecs.encode(pickle.dumps(sku_model_lm), \"base64\").decode()\n",
    "#     result = pd.concat([df[['item_sku_id','dt','valid_flag','model_type','target_qtty','prediction']+X_SCHEMA_SKU],pd.DataFrame({'second_prediction':list(map(lambda x:x[0],xuni_ls))})],axis=1)\n",
    "    \n",
    "    yichang_dt = df.groupby('dt').apply(lambda x:yichang(x)).reset_index().rename(columns={0:'dt_flag'})\n",
    "    yichang_dt = list(yichang_dt.loc[yichang_dt['dt_flag']==1,'dt'])\n",
    "    \n",
    "    result = pd.concat([df[['item_sku_id','dt','valid_flag','model','model_type','max_qtty','target_qtty','prediction']+X_SCHEMA_SKU],pd.DataFrame({'second_prediction':list(xuni_ls)})],axis=1)\n",
    "    result.loc[result['dt'].isin(yichang_dt)==False,'second_prediction'] = result.loc[result['dt'].isin(yichang_dt)==False,'prediction']\n",
    "    \n",
    "    result['huber_model'] = huber_m \n",
    "    result = result[['item_sku_id','dt','valid_flag','model_type','model','huber_model','max_qtty','target_qtty']+X_SCHEMA_SKU+['prediction','second_prediction']]\n",
    "    \n",
    "    # 销量要小于等于历史最大销量，且大于等于0\n",
    "    result.loc[result['second_prediction']>df.iloc[0]['max_qtty'],'second_prediction'] = df.iloc[0]['max_qtty']\n",
    "    result.loc[result['second_prediction']<0,'second_prediction'] = 0\n",
    "    \n",
    "    return result.to_dict(orient= 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71804"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机森林做huber\n",
    "second_result = second_df.filter(F.col('model_type')=='rf').rdd.map(lambda row: ((row['item_sku_id']), row)).groupByKey()\\\n",
    "    .flatMap(lambda row : smooth(row))\n",
    "\n",
    "second_result2 = spark.createDataFrame(second_result.map(format_result_sku2), schema=SCHEMA_OUTPUT_SKU2)\n",
    "second_result2.cache()\n",
    "second_result2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265866"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机森林和其他的再union到一起\n",
    "second_result3 = second_result2.select('item_sku_id','dt','valid_flag','model_type','model','huber_model','max_qtty','target_qtty','target_price','prediction','second_prediction')\\\n",
    "                               .union(second_df.filter(F.col('model_type')!='rf').withColumn('huber_model',F.lit(None))\\\n",
    "                                               .select('item_sku_id','dt','valid_flag','model_type','model','huber_model','max_qtty','target_qtty','target_price','prediction')\\\n",
    "                                               .withColumn('second_prediction',F.col('prediction')))\n",
    "\n",
    "\n",
    "\n",
    "second_result3.cache()\n",
    "second_result3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入计算mape的列\n",
    "second_result3 = second_result3.withColumn('percent',F.abs(F.col('target_qtty')-F.col('prediction'))/F.when(F.col('target_qtty')==0,F.lit(1)).otherwise(F.col('target_qtty')))\\\n",
    "                               .withColumn('second_percent',F.abs(F.col('target_qtty')-F.col('second_prediction'))/F.when(F.col('target_qtty')==0,F.lit(1)).otherwise(F.col('target_qtty')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_result3.write.mode('overwrite').format('orc').saveAsTable('dev.dev_xianzhi_model_data_6_result_zou')\n",
    "spark.sql('''ALTER TABLE dev.dev_xianzhi_model_data_6_result_zou set TBLPROPERTIES ('author'='zoushuhan') ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265866"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_result3 = spark.sql('''select * from dev.dev_xianzhi_model_data_6_result_zou ''')\n",
    "second_result3.cache()\n",
    "second_result3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   max(dt)|\n",
      "+----------+\n",
      "|2019-07-11|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_result3.filter(F.col('valid_flag')==2).agg(F.max('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   max(dt)|\n",
      "+----------+\n",
      "|2019-08-25|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_result3.agg(F.max('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|avg(second_percent)|\n",
      "+-------------------+\n",
      "| 3.5730705304489088|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sum(result_table2['percent'])/len(result_table2)\n",
    "second_result3.agg(F.mean('second_percent')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|avg(second_percent)|\n",
      "+-------------------+\n",
      "| 2.9055066070557585|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sum(result_table2.loc[result_table2['valid_flag']==2,'percent'])/len(result_table2.loc[result_table2['valid_flag']==2))\n",
    "second_result3.filter(F.col('valid_flag')==2).agg(F.mean('second_percent')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|count(second_percent)|avg(second_percent)|\n",
      "+---------------------+-------------------+\n",
      "|                 4880| 0.8720085390860289|\n",
      "+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sum(result_table2.loc[(result_table2['valid_flag']==2)&(result_table2['target_qtty']>=10),'percent'])/len(result_table2.loc[(result_table2['valid_flag']==2)&(result_table2['target_qtty']>=10)])\n",
    "second_result3.filter(F.col('valid_flag')==2).filter(F.col('target_qtty')>=10).agg(F.count('second_percent'),F.mean('second_percent')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+\n",
      "|count(second_percent)|avg(second_percent)|\n",
      "+---------------------+-------------------+\n",
      "|                 4843| 0.7896462753338485|\n",
      "+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_result3.filter(F.col('valid_flag')==2).filter(F.col('target_qtty')>=10)\\\n",
    "    .filter(F.col('item_sku_id').isin(['6258780','100003741320','407901','2918980','6083253'])==False).agg(F.count('second_percent'),F.mean('second_percent')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(mape)|\n",
      "+------------------+\n",
      "|0.8316104815208014|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按sku算mape再平均\n",
    "second_result3.filter(F.col('valid_flag')==2).filter(F.col('target_qtty')>=10)\\\n",
    "    .filter(F.col('item_sku_id').isin(['6258780','100003741320','407901','2918980','6083253'])==False).groupby('item_sku_id').agg(F.count('second_percent'),F.mean('second_percent').alias('mape')).agg(F.mean('mape')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|             wmape|\n",
      "+------------------+\n",
      "|0.5273395108177168|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sum(abs(result_table2.loc[(result_table2['valid_flag']==2)&(result_table2['target_qtty']>=10),'target_qtty']-\n",
    "#         result_table2.loc[(result_table2['valid_flag']==2)&(result_table2['target_qtty']>=10),'prediction']))/sum(result_table2.loc[(result_table2['valid_flag']==2)&(result_table2['target_qtty']>=10),'target_qtty'])\n",
    "wmape = second_result3.withColumn('cha',F.abs(F.col('target_qtty')-F.col('second_prediction')))\n",
    "sum_qtty = wmape.filter(F.col('valid_flag')==2).filter(F.col('target_qtty')>=10)\\\n",
    "                .agg(F.sum('target_qtty')).collect()[0][0]\n",
    "\n",
    "wmape.filter(F.col('valid_flag')==2).filter(F.col('target_qtty')>=10)\\\n",
    "     .agg((F.sum('cha')/sum_qtty).alias('wmape')).show()\n",
    "\n",
    "# second = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|             wmape|\n",
      "+------------------+\n",
      "|0.5163970197688419|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sum(abs(result_table2.loc[(result_table2['valid_flag']==2)&(result_table2['target_qtty']>=10),'target_qtty']-\n",
    "#         result_table2.loc[(result_table2['valid_flag']==2)&(result_table2['target_qtty']>=10),'prediction']))/sum(result_table2.loc[(result_table2['valid_flag']==2)&(result_table2['target_qtty']>=10),'target_qtty'])\n",
    "wmape = second_result3.withColumn('cha',F.abs(F.col('target_qtty')-F.col('second_prediction')))\n",
    "sum_qtty = wmape.filter(F.col('valid_flag')==2).filter(F.col('target_qtty')>=10)\\\n",
    "                .filter(F.col('item_sku_id').isin(['6258780','100003741320','407901','2918980','6083253'])==False)\\\n",
    "                .agg(F.sum('target_qtty')).collect()[0][0]\n",
    "\n",
    "wmape.filter(F.col('valid_flag')==2).filter(F.col('target_qtty')>=10)\\\n",
    "     .filter(F.col('item_sku_id').isin(['6258780','100003741320','407901','2918980','6083253'])==False)\\\n",
    "     .agg((F.sum('cha')/sum_qtty).alias('wmape')).show()\n",
    "\n",
    "# second = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|model_type|count(percent)|\n",
      "+----------+--------------+\n",
      "|        rf|          1536|\n",
      "|        lm|           633|\n",
      "|   xgboost|          1788|\n",
      "|   prophet|           886|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_result3.filter(F.col('valid_flag')==2).filter(F.col('target_qtty')>=10).filter(F.col('item_sku_id').isin(['6258780','100003741320','407901','2918980','6083253'])==False)\\\n",
    "              .groupby('model_type').agg(F.count('percent')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6258780 prophet 有异常，不过sku其中有一段下柜的跳跃  \n",
    "100003741320  训练集中本身都是39-40元卖200-400个，但测试集中41元就只能卖10-20个  \n",
    "407901  测试集中很多天都是价格高销量高，反向数据多，需要剔除  \n",
    "2918980 sku本身波动太大，同样的价格前后销量差距很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
